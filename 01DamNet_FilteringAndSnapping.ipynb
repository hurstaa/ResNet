{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10745787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Written 8/11/2022 (updated 6/27/2024) to automate the filtering process and assign dam order to all dams in the nation.\n",
    "# The starting file for this script is a csv file of the NID database dams for the entire nation \n",
    "# downloaded from https://nid.sec.usace.army.mil/#/downloads. You should be able to run this for only a subset\n",
    "# of the dams as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce31f4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with Python 3 and ArcGIS Pro v 3.2.2. User needs and ArcGIS Pro installation to use the arcpy package in this code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8efda67",
   "metadata": {},
   "source": [
    "# 1. Import databases and filter them. Combine databases as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "876b39f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTables is not installed. No support for HDF output.\n",
      "SQLalchemy is not installed. No support for SQL output.\n",
      "ArcGIS Python environment found at: C:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\n",
      "Successfully imported arcpy!\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import math\n",
    "from simpledbf import Dbf5\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "# Import arcpy packages\n",
    "import archook\n",
    "\n",
    "def find_arcgis_python():\n",
    "    \"\"\"Attempts to find the ArcGIS Pro Python environment automatically.\"\"\"\n",
    "    arcgis_base = r\"C:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\"\n",
    "    \n",
    "    if os.path.exists(arcgis_base):\n",
    "        # Locate the ArcGIS Pro environment dynamically\n",
    "        envs = glob.glob(os.path.join(arcgis_base, \"arcgispro-py3*\"))\n",
    "        if envs:\n",
    "            return envs[0]  # Return the first match (should be the correct environment)\n",
    "    \n",
    "    return None  # If not found, return None\n",
    "\n",
    "arcgis_env = find_arcgis_python()\n",
    "\n",
    "if arcgis_env:\n",
    "    sys.path.append(arcgis_env)\n",
    "    print(f\"ArcGIS Python environment found at: {arcgis_env}\")\n",
    "else:\n",
    "    print(\"ArcGIS Pro Python environment not found!\")\n",
    "\n",
    "# Now try importing arcpy\n",
    "try:\n",
    "    archook.arcpy = arcgis_env\n",
    "    print(\"Successfully imported arcpy!\")\n",
    "except ImportError:\n",
    "    print(\"Failed to import arcpy. Ensure ArcGIS Pro is installed and accessible.\")\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns',None) #Set display to show all columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae555cdc",
   "metadata": {},
   "source": [
    "#### Load data here. If you have downloaded the repository locally, all files are available within the repository folder. You will need to unzip the geodatabase to that folder before running. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6357611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of NID in database: (91886, 84)\n",
      "Number of USBR in database: (266, 17)\n",
      "Number of USACE in database: (465, 17)\n",
      "Number of sites in database: (730, 17)\n",
      "Number of river mouths in database: (143, 17)\n"
     ]
    }
   ],
   "source": [
    "## Load data. See the files included with the report for formatting. Also load in the raw files (not the cross-ref files) and cross-ref\n",
    "## in the code.\n",
    "\n",
    "# Load NID data (NID downloaded 06/19/2024)\n",
    "NIDs = pd.read_csv('Inputs/ExternallyDownloadedDatasets/NID2024.csv', header=1, low_memory=False) \n",
    "\n",
    "# Load removed dams file, if using\n",
    "removed = pd.read_csv('Inputs/ExternallyDownloadedDatasets/RemovedDams.csv')\n",
    "\n",
    "# Load GeoDAR data to use as location where no NID location (GeoDAR v11)\n",
    "geoDAR = pd.read_csv('Inputs/ExternallyDownloadedDatasets/GeoDAR_v11_dams.csv')\n",
    "# geoDAR = pd.read_csv('E:/ResSed/MediumResolution_DamLinkages/Manuscript/FinalInputFiles/GeoDAR_crossref.csv')\n",
    "\n",
    "#Load GDAT data to use for additional storage and year complete (GDAT v1)\n",
    "GDAT = pd.read_csv('Inputs/ExternallyDownloadedDatasets/GDAT_v1_dams.csv', low_memory=False)\n",
    "\n",
    "# Load the dam attribute and location file for if you have sites with additional data or better locations that you want\n",
    "# included. This can include completion year or storage data. This is where you tag dams for the SiteTag function.\n",
    "sites = pd.read_csv('Inputs/resnet_attributes.csv') \n",
    "\n",
    "# Load NHD Plus Medium Resolution flowline shapefile (NHDPlus v2); This needs to be a path on your computer\n",
    "# NHDFlowline = 'NHDFlowline_Network_NHDPlus_Countries.gdb/NHDFlowline_Network_NHDPlus_Countries'  # Path for NHD flowline shapefile\n",
    "\n",
    "# Load GRanD data. (GRanD v1.3 with modifications to locations that places GRanD on NHD Flowlines)\n",
    "GRanD = pd.read_csv('Inputs/ExternallyDownloadedDatasets/GRanD_dams_v1_3.csv')\n",
    "\n",
    "# Load the most recent version of DamNet to transfer ShortIDs for the new run for consistency.\n",
    "resnet = pd.read_csv(r\"ResNet.csv\")\n",
    "\n",
    "# Load the dataset cross-reference file.\n",
    "crossref = pd.read_csv(r'resnet_datacrossreference.csv')\n",
    "\n",
    "# Assign output folder directory where you want to save the output files.\n",
    "out_folder = 'Outputs' # Write full path to this folder for arcgis outputs to be saved properly\n",
    "\n",
    "# Print original numbers of dams in various databases for tracking:\n",
    "print('Original number of NID in database:',NIDs.shape)\n",
    "print('Number of USBR in database:',sites.loc[sites.IsUSBR==1].shape)\n",
    "print('Number of USACE in database:',sites.loc[sites.IsUSACE==1].shape)\n",
    "print('Number of sites in database:',sites.loc[sites.IsSite==1].shape)\n",
    "print('Number of river mouths in database:',sites.loc[sites.IsRiverMth==1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "773e114b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of raw NIDs with no locations: (2, 84)\n"
     ]
    }
   ],
   "source": [
    "# Count how many dams have lat/long as 0\n",
    "noLoc = NIDs[(NIDs.Latitude == 0) | (NIDs.Longitude == 0)]\n",
    "\n",
    "print('Number of raw NIDs with no locations:', noLoc.shape)\n",
    "#noLoc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16571b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean NID data to only keep the columns that we need\n",
    "# FOR 2024 NID FILE: Column headers may change between versions of NID downloads. Check this if re-running with a new NID download\n",
    "\n",
    "# Rename columns\n",
    "new_cols = ['Dam_Name','Other_Dam','Former_Name','NID','OtherStructureID','FederalID','Owner_Name','OwnerTypes','PrimaryOwnerType','NumStruct','AssStruct','Designer','NonFedDam','PrimaryPurp', 'Purp','SourceAgency','StateorFedID','Latitude','Longitude','State','County','City','DisttoCity','River','CongressDist','AmInd','SecLoc','StateReg','Juris','Agency','StatePerm','StateInsp','StateEnforce','FedReg','FedOwner','FedFunding','FedDesign','FedConst','FedReg','FedInsp','FedOps','FedOther','SecAg','NRCS','PrimDamType','DamTypes','CoreTypes','Foundation','Dam_Height','HydraulicHeight','StructHeight','NID_Height','NIDHeightCat','Dam_Length','Volume','Year_Compl','YearCompCat','Year_Modif','NID_Storag','Max_Storag','Normal_Sto','Surface_Ar','Drainage_A','Max_Discha','SpillwayType','SpillWidth','NumLocks','LengthLocks','LockWidth','LengthSecondLock','SecondLockWidth','OutletGate','DataUpdated','LastInspection','InspectionFreq','HazardClass','CondAss','CondAssDate','OpStat','OpStatDate','EAPPrep','EAPRev','InundationMap','URL']\n",
    "NIDs.columns = new_cols\n",
    "\n",
    "\n",
    "# Filter and rename variables\n",
    "NIDs = NIDs[['Dam_Name','Other_Dam','NID','OtherStructureID','FederalID','Longitude','Latitude','State','River','Owner_Name','OwnerTypes','Year_Compl','Year_Modif','NID_Height','Dam_Length','NID_Storag','Max_Storag','Normal_Sto','Surface_Ar','Drainage_A','Max_Discha','PrimaryPurp','Purp','PrimDamType','NumLocks','LengthLocks','LockWidth','LengthSecondLock','SecondLockWidth']]\n",
    "#Strip white spaces from NID IDs\n",
    "NIDs['NID'] = NIDs['NID'].str.replace(\" \", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74a41717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign Short ID to NID; If the dam was in a previous version of ResNet, keep the Short ID the same\n",
    "NIDs = pd.merge(NIDs, resnet[['NID','ShortID']],how = 'left', on = 'NID')\n",
    "\n",
    "# Merge input datasets with cross-reference file to transfer NID\n",
    "geoDAR = pd.merge(geoDAR,crossref[['NID','GeoDAR_id_v11']],how = 'left',left_on='id_v11',right_on = 'GeoDAR_id_v11')\n",
    "GDAT = pd.merge(GDAT,crossref[['NID','GDAT_Feature_ID']],how = 'left', left_on = 'Feature_ID', right_on = 'GDAT_Feature_ID')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafb5854",
   "metadata": {},
   "source": [
    "#### Move and delete NID dams using the spatial edits file if so desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26833d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length to delete: (211, 17)\n",
      "NID database size: (91861, 46)\n",
      "Number of Moved dams in database: (2595, 46)\n",
      "Number of sites: (1015, 46)\n"
     ]
    }
   ],
   "source": [
    "# Use spatial edit file to change lat/long of moved dams and remove deleted dams. If you do not wish to modify original NID, skip this.\n",
    "delete = sites.loc[sites.Deleted == 1]\n",
    "move = sites.loc[sites.Moved == 1]\n",
    "\n",
    "NIDs = NIDs[~NIDs['NID'].isin(delete['NID'])] # Drop deleted NIDs\n",
    "\n",
    "NID_join = pd.merge(NIDs, move, on='NID', how = 'outer') # Add moved NIDs to NID dataframe\n",
    "\n",
    "\n",
    "# Change the Latitude and Longitude fields of the NIDs you wish to move\n",
    "NID_join.loc[NID_join.Moved == 1,'Latitude'] = NID_join.Lat \n",
    "NID_join.loc[NID_join.Moved == 1, 'Longitude'] = NID_join.Long \n",
    "\n",
    "NID_join.Moved = NID_join.Moved.fillna(0)\n",
    "\n",
    "print('Length to delete:', delete.shape)\n",
    "print('NID database size:',NID_join.shape) # New database size after incorporating dam attribute file\n",
    "print('Number of Moved dams in database:', NID_join.loc[NID_join.Moved == 1].shape) \n",
    "print('Number of sites:',NID_join.loc[NID_join.IsSite==1].shape)\n",
    "\n",
    "# Combine duplicate ShortID fields from join\n",
    "NID_join['ShortID'] = NID_join.ShortID_x\n",
    "NID_join.loc[NID_join['ShortID'].isna(),'ShortID'] = NID_join.ShortID_y\n",
    "\n",
    "NID_join = NID_join.drop(columns = ['ShortID_x','ShortID_y'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b3f803d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaNs with 0\n",
    "NID_join.NID_Storag = NID_join.NID_Storag.fillna(0)\n",
    "NID_join.Max_Storag = NID_join.Max_Storag.fillna(0)\n",
    "NID_join.Normal_Sto = NID_join.Normal_Sto.fillna(0)\n",
    "NID_join.IsUSBR = NID_join.IsUSBR.fillna(0)\n",
    "NID_join.IsUSACE = NID_join.IsUSACE.fillna(0)\n",
    "NID_join.IsSite = NID_join.IsSite.fillna(0)\n",
    "NID_join.IsRiverMth = NID_join.IsRiverMth.fillna(0)\n",
    "NID_join.Moved = NID_join.Moved.fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed08c91a",
   "metadata": {},
   "source": [
    "#### Remove duplicate NIDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27969ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size after removing duplicates and joining to site file: (91033, 46)\n",
      "Number of sites in database: (726, 46)\n",
      "Number of Reclamation dams in database: (262, 46)\n",
      "Number of USACE dams in database: (465, 46)\n",
      "Number of Moved dams in database: (2233, 46)\n",
      "Number of rivers in database: (143, 46)\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate NIDs. Keep dam with largest reported storage data.\n",
    "NID_join = NID_join.drop(index=NID_join.loc[NID_join.OtherStructureID.notnull()].index)\n",
    "\n",
    "# A couple of dams don't have Other Structure ID but are duplicates. Filter those by storage. Should only be 2\n",
    "# Sort the data by descending max storage\n",
    "NID_join = NID_join.sort_values('NID_Storag', ascending = False)\n",
    "\n",
    "# Remove duplicate NIDs, keeping the first value aka the biggest capacity\n",
    "bool_series = NID_join['NID'].duplicated()\n",
    "NID_join = NID_join[~bool_series]\n",
    "\n",
    "NID_join = NID_join.reset_index()\n",
    "\n",
    "# Print checks\n",
    "print('Size after removing duplicates and joining to site file:',NID_join.shape) # New database size after removing duplicate NIDs\n",
    "print('Number of sites in database:',NID_join.loc[(NID_join.IsSite == 1)].shape)\n",
    "print('Number of Reclamation dams in database:', NID_join.loc[NID_join.IsUSBR == 1].shape)\n",
    "print('Number of USACE dams in database:', NID_join.loc[NID_join.IsUSACE == 1].shape) \n",
    "print('Number of Moved dams in database:', NID_join.loc[NID_join.Moved == 1].shape)\n",
    "print('Number of rivers in database:', NID_join.loc[NID_join.IsRiverMth==1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8991791f-db68-414d-935d-c80df0d41334",
   "metadata": {},
   "source": [
    "#### Locate and populate lock dams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fb4f551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate IsLock column\n",
    "# Replace null values with 0\n",
    "# Assign 1 to any NID field with lock information\n",
    "NID_join.NumLocks = NID_join.NumLocks.fillna(0)\n",
    "NID_join.LengthLocks = NID_join.LengthLocks.fillna(0)\n",
    "NID_join.LockWidth = NID_join.LockWidth.fillna(0)\n",
    "NID_join.LengthSecondLock = NID_join.LengthSecondLock.fillna(0)\n",
    "NID_join.SecondLockWidth = NID_join.SecondLockWidth.fillna(0)\n",
    "NID_join.loc[(NID_join.NumLocks>0) & (NID_join.NumLocks<10),'IsLock'] = 1 \n",
    "NID_join.loc[(NID_join.LengthLocks>0),'IsLock'] = 1\n",
    "NID_join.loc[(NID_join.LockWidth>0),'IsLock'] = 1\n",
    "NID_join.loc[(NID_join.LengthSecondLock>0),'IsLock'] = 1\n",
    "NID_join.loc[(NID_join.SecondLockWidth>0),'IsLock'] = 1\n",
    "\n",
    "# Search by name containing word 'Lock'\n",
    "NID_join['Dam_Name'] = NID_join['Dam_Name'].fillna('') # First fill NA names with empty strings\n",
    "NID_join.loc[NID_join['Dam_Name'].str.contains('Lock '), 'IsLock'] = 1 \n",
    "\n",
    "# Set GA01804 and MO20537 to IsLock = 0 because are not locks but have lock in name\n",
    "NID_join.loc[NID_join['NID'] == 'GA01804', 'IsLock'] = 0\n",
    "NID_join.loc[NID_join['NID'] == 'MO20537', 'IsLock'] = 0\n",
    "\n",
    "# Fill null values with 0\n",
    "NID_join.IsLock = NID_join.IsLock.fillna(0)\n",
    "\n",
    "# Drop columns we no longer need.\n",
    "NID_join = NID_join.drop(['NumLocks','LengthLocks','LockWidth','LengthSecondLock','SecondLockWidth','Deleted'],axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c9e060",
   "metadata": {},
   "source": [
    "#### Filter dams by name to remove any dam names that contain Spillway, Levee, Sewage, Treatment, Auxiliary, or Remedial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a47a511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size before filtering: (91033, 40)\n",
      "Size after filtering by name: (90562, 40)\n",
      "Number of sites in database: (726, 40)\n",
      "Number of Reclamation dams in database: (262, 40)\n",
      "Number of USACE dams in database: (465, 40)\n"
     ]
    }
   ],
   "source": [
    "print('Size before filtering:', NID_join.shape)\n",
    "\n",
    "# Filter by name: Filter the dams that are NOT moved in the dam attributes file\n",
    "filters = \"Spillway|Levee|Sewage|Treatment|Auxiliary|Remedial\"\n",
    "\n",
    "# Only filter non-sites\n",
    "NIDs_filtered = NID_join.drop(index=NID_join.loc[NID_join.Dam_Name.str.contains(filters)==True].loc[(NID_join.Moved == 0)].index)\n",
    "\n",
    "NID_join = NIDs_filtered\n",
    "\n",
    "print('Size after filtering by name:', NIDs_filtered.shape) # New database size after filtering by name\n",
    "print('Number of sites in database:', NID_join.loc[NID_join.IsSite == 1].shape) \n",
    "print('Number of Reclamation dams in database:', NID_join.loc[NID_join.IsUSBR == 1].shape)\n",
    "print('Number of USACE dams in database:', NID_join.loc[NID_join.IsUSACE == 1].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b7e560",
   "metadata": {},
   "source": [
    "#### Join remaining supplementary files and modify latitude and longitude appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18036348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of GRanD: (1901, 23)\n"
     ]
    }
   ],
   "source": [
    "## Join to GRanD dams\n",
    "\n",
    "# Filter so only dams in the United States\n",
    "GRanD = GRanD[(GRanD['COUNTRY'].str.contains('United States')== True)|(GRanD['SEC_CNTRY'].str.contains('United States')==True)]\n",
    "GRanD = GRanD[GRanD['ADMIN_UNIT'].str.contains('Alaska') == False]\n",
    "GRanD = GRanD[GRanD['ADMIN_UNIT'].str.contains('Hawaii') == False]\n",
    "GRanD = GRanD[GRanD['ADMIN_UNIT'].str.contains('Puerto Rico') == False]\n",
    "                \n",
    "# Filter out columns we don't want\n",
    "GRanD = GRanD.drop(columns = ['RIVER','ALT_RIVER','MAIN_BASIN','SUB_BASIN','NEAR_CITY','ALT_CITY','SEC_ADMIN','COUNTRY','SEC_CNTRY','ALT_YEAR','ALT_HGT_M','DAM_LEN_M','ALT_LEN_M','AREA_SKM','AREA_POLY','AREA_REP','AREA_MAX','AREA_MIN','CAP_MAX','CAP_REP','CAP_MIN','DEPTH_M','DIS_AVG_LS','DOR_PC','ELEV_MASL','CATCH_SKM','CATCH_REP','DATA_INFO','USE_IRRI','USE_ELEC','USE_SUPP','USE_FCON','USE_RECR','USE_NAVI','USE_FISH','USE_PCON','USE_LIVE','USE_OTHR','MAIN_USE','LAKE_CTRL','MULTI_DAMS','TIMELINE','COMMENTS','URL','QUALITY','EDITOR','POLY_SRC'])\n",
    "\n",
    "print('Size of GRanD:', GRanD.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "870da7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size before joining to GRanD: (90562, 40)\n",
      "Size after joining to GRanD: (90624, 70)\n",
      "Number of sites in database: (726, 70)\n",
      "Number of Reclamation dams in database: (262, 70)\n",
      "Number of USACE dams in database: (465, 70)\n",
      "Number of GRanD in database: (1898, 70)\n"
     ]
    }
   ],
   "source": [
    "# Find matches between GRanD NID and NID NID; outer join should keep GRanD dams that weren't in NID\n",
    "GRanD_join = pd.merge(NID_join, GRanD, on='NID', how='outer', suffixes = ('_NID','_GRanD'))\n",
    "\n",
    "# Combine output columns that were split in the join\n",
    "GRanD_join.loc[GRanD_join.ShortID_GRanD.notnull(), 'ShortID'] = GRanD_join.ShortID_GRanD\n",
    "GRanD_join.loc[GRanD_join.ShortID.isnull(), 'ShortID'] = GRanD_join.ShortID_NID\n",
    "\n",
    "# Assign 0 to NaNs\n",
    "GRanD_join.loc[GRanD_join.IsSite_NID.isna(),'IsSite_NID'] = 0\n",
    "GRanD_join['IsSite'] = GRanD_join['IsSite_NID']\n",
    "GRanD_join.loc[GRanD_join.IsUSBR_NID.isna(),'IsUSBR_NID'] = 0\n",
    "GRanD_join['IsUSBR'] = GRanD_join['IsUSBR_NID']\n",
    "GRanD_join.loc[GRanD_join.IsUSACE_NID.isna(),'IsUSACE_NID'] = 0\n",
    "GRanD_join['IsUSACE'] = GRanD_join['IsUSACE_NID']\n",
    "\n",
    "# Create a GRanD lat/long field that takes the GRanD lat/long preferentially. These fields are called NewX and NewY for our manual placements.\n",
    "GRanD_join['LAT_GRAND'] = GRanD_join['NewY']\n",
    "GRanD_join['LONG_GRAND'] = GRanD_join['NewX']\n",
    "\n",
    "# Set null values to -999\n",
    "GRanD_join.LAT_GRAND = GRanD_join.LAT_GRAND.fillna(-999)\n",
    "GRanD_join.LONG_GRAND = GRanD_join.LONG_GRAND.fillna(-999)\n",
    "\n",
    "# Create a new field for lat/long that takes the GRanD lat/long if not -999 and takes the original lat/long if -999\n",
    "GRanD_join['LAT_JOIN'] = GRanD_join.LAT_GRAND\n",
    "GRanD_join['LONG_JOIN'] = GRanD_join.LONG_GRAND\n",
    "GRanD_join.loc[GRanD_join.LAT_GRAND == -999, 'LAT_JOIN'] = GRanD_join.Latitude\n",
    "GRanD_join.loc[GRanD_join.LONG_GRAND == -999, 'LONG_JOIN'] = GRanD_join.Longitude\n",
    "\n",
    "#Drop any GRanD dams that do not have an NID\n",
    "GRanD_join = GRanD_join[GRanD_join['NID'].str.strip() != \"\"]\n",
    "\n",
    "print('Size before joining to GRanD:',NID_join.shape)\n",
    "print('Size after joining to GRanD:',GRanD_join.shape) # New database size after adding GRanD dams\n",
    "print('Number of sites in database:',GRanD_join.loc[GRanD_join.IsSite == 1].shape) \n",
    "print('Number of Reclamation dams in database:', GRanD_join.loc[GRanD_join.IsUSBR == 1].shape)\n",
    "print('Number of USACE dams in database:', GRanD_join.loc[GRanD_join.IsUSACE == 1].shape)\n",
    "print('Number of GRanD in database:', GRanD_join.loc[GRanD_join.IsGRanD == 1].shape) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00a4b546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dams in database after joining to removed dams: (90667, 93)\n"
     ]
    }
   ],
   "source": [
    "# Find matches between Removed Dams and NID by NID; outer join should keep Removed dams that weren't in NID. \n",
    "# If you do not want to use a removed dams file, skip this step.\n",
    "GRanD_join = pd.merge(GRanD_join, removed, on='NID', how='outer', suffixes = ('_join','_rem'))\n",
    "\n",
    "# Create a GRanD lat/long field that takes the lat/long value from the two joins that is not -999 (the maximum)\n",
    "GRanD_join['LAT_Rem'] = GRanD_join['DamLatitud']\n",
    "GRanD_join['LONG_Rem'] = GRanD_join['DamLongitu']\n",
    "\n",
    "# Set null values to -999\n",
    "GRanD_join.LAT_Rem = GRanD_join.LAT_Rem.fillna(-999)\n",
    "GRanD_join.LONG_Rem = GRanD_join.LONG_Rem.fillna(-999)\n",
    "\n",
    "# Make GRanD lat/long null for GRanD and manually placed moved dams. This preferentially keeps their locations.\n",
    "GRanD_join.loc[GRanD_join.IsGRanD == 1, 'LAT_Rem'] = -999\n",
    "GRanD_join.loc[GRanD_join.IsGRanD == 1, 'LONG_Rem'] = -999                                                                                    \n",
    "\n",
    "GRanD_join.loc[GRanD_join.Moved == 1, 'LAT_Rem'] = -999\n",
    "GRanD_join.loc[GRanD_join.Moved == 1, 'LONG_Rem'] = -999\n",
    "                           \n",
    "# Create a new field for lat/long that takes the Removed dam lat/long if not null and takes the original lat/long if null\n",
    "GRanD_join.loc[GRanD_join.LAT_Rem != -999, 'LAT_JOIN'] = GRanD_join.LAT_Rem\n",
    "GRanD_join.loc[GRanD_join.LONG_Rem != -999, 'LONG_JOIN'] = GRanD_join.LONG_Rem\n",
    "\n",
    "print('Number of dams in database after joining to removed dams:',GRanD_join.shape) # New database size after adding removed dams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdeb3987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dams in database: (90667, 97)\n",
      "Number of locations from GeoDAR: 4281\n"
     ]
    }
   ],
   "source": [
    "# Join to GeoDAR and add GeoDAR locations where able\n",
    "NID_geoDAR = pd.merge(GRanD_join, geoDAR[['NID','lat','lon']], on='NID', how='left')\n",
    "# NID_geoDAR = pd.merge(GRanD_join, geoDAR[['NID','NewX','NewY']], on='NID', how='left')\n",
    "\n",
    "# Create a GRanD lat/long field that takes the lat/long value from the two joins that is not -999 (the maximum)\n",
    "NID_geoDAR['LAT_Geo'] = NID_geoDAR['lat']\n",
    "NID_geoDAR['LONG_Geo'] = NID_geoDAR['lon']\n",
    "# NID_geoDAR['LAT_Geo'] = NID_geoDAR['NewY_y']\n",
    "# NID_geoDAR['LONG_Geo'] = NID_geoDAR['NewX_y']\n",
    "\n",
    "# Set null values to -999\n",
    "NID_geoDAR.LAT_Geo = NID_geoDAR.LAT_Geo.fillna(-999)\n",
    "NID_geoDAR.LONG_Geo = NID_geoDAR.LONG_Geo.fillna(-999)\n",
    "\n",
    "# Make GRanD lat/long null for GRanD and manually placed moved dams. This preferentially keeps their locations\n",
    "NID_geoDAR.loc[NID_geoDAR.IsGRanD == 1, 'LAT_Geo'] = -999\n",
    "NID_geoDAR.loc[NID_geoDAR.IsGRanD == 1, 'LONG_Geo'] = -999                                                                                    \n",
    "\n",
    "NID_geoDAR.loc[NID_geoDAR.Moved == 1, 'LAT_Geo'] = -999\n",
    "NID_geoDAR.loc[NID_geoDAR.Moved == 1, 'LONG_Geo'] = -999\n",
    "                                                                                    \n",
    "# Create a new field for lat/long that takes the GeoDAR lat/long if not null and takes the original lat/long if null\n",
    "NID_geoDAR.loc[NID_geoDAR.LAT_Rem != -999, 'LAT_JOIN'] = NID_geoDAR.LAT_Geo\n",
    "NID_geoDAR.loc[NID_geoDAR.LONG_Rem != -999, 'LONG_JOIN'] = NID_geoDAR.LONG_Geo\n",
    "\n",
    "print('Number of dams in database:', NID_geoDAR.shape)\n",
    "print('Number of locations from GeoDAR:', len(NID_geoDAR.loc[NID_geoDAR['LAT_Geo']!=-999]))\n",
    "\n",
    "GRanD_join = NID_geoDAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba1d76ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join to GDAT and get dam height, storage, and year completed data.\n",
    "\n",
    "GRanD_join = pd.merge(GRanD_join,GDAT[['NID','Year_Fin','Volume_Max','Height']],on='NID',how='left')\n",
    "GRanD_join.Year_Fin = pd.to_numeric(GRanD_join['Year_Fin'],errors='coerce') #convert string dates to integers from GDAT\n",
    "GRanD_join.Height = pd.to_numeric(GRanD_join['Height'],errors='coerce') #convert Height to integers from GDAT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c69b861",
   "metadata": {},
   "source": [
    "#### Filter based on storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6603f010",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size before storage filtering: (90667, 100)\n",
      "Size after storage filtering: (89829, 104)\n",
      "Number of sites in database: (726, 104)\n",
      "Number of Reclamation dams in database: (262, 104)\n",
      "Number of USACE dams in database: (465, 104)\n",
      "Number of GRanD in database: (1898, 104)\n"
     ]
    }
   ],
   "source": [
    "# Assign 0 to IsGRanD non-GRanD dams\n",
    "GRanD_join.IsGRanD = GRanD_join.IsGRanD.fillna(0)\n",
    "\n",
    "# Filter based on storage\n",
    "print('Size before storage filtering:', GRanD_join.shape)\n",
    "\n",
    "# Convert fields to m3\n",
    "GRanD_join['NIDStor_m3'] = GRanD_join.NID_Storag*1233.48 #AF to m3\n",
    "GRanD_join['GRanDCapm3'] = GRanD_join.CAP_MCM * (1e6) # Convert million cubic meters to cubic m\n",
    "GRanD_join['Volume_Max'] = GRanD_join.Volume_Max * (1e6) # Convert million cubic meters to cubic m\n",
    "\n",
    "# Fill all null storage values in all fields with 0\n",
    "GRanD_join.NIDStor_m3 = GRanD_join.NIDStor_m3.fillna(0)\n",
    "GRanD_join.GRanDCapm3 = GRanD_join.GRanDCapm3.fillna(0)\n",
    "GRanD_join.Capacity_m3 = GRanD_join.Capacity_m3.fillna(0)\n",
    "GRanD_join.OrigCap_m3 = GRanD_join.OrigCap_m3.fillna(0)\n",
    "GRanD_join.Volume_Max = GRanD_join.Volume_Max.fillna(0)\n",
    "\n",
    "# Replace NaNs with 0\n",
    "GRanD_join.IsUSBR = GRanD_join.IsUSBR.fillna(0)\n",
    "GRanD_join.IsUSACE = GRanD_join.IsUSACE.fillna(0)\n",
    "GRanD_join.IsSite = GRanD_join.IsSite.fillna(0)\n",
    "GRanD_join.IsRiverMth = GRanD_join.IsRiverMth.fillna(0)\n",
    "GRanD_join.Moved = GRanD_join.Moved.fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "# Create column that takes the maximum of all of the storage values for a given field. This is NID_Storag in the NID table.\n",
    "GRanD_join['MaxStor_m3'] = GRanD_join['NIDStor_m3']\n",
    "\n",
    "# Set GRanD_join storage sources to initially be NID\n",
    "# The outcome of this is that for each storage value reported, you have a source and a year that storage value represents\n",
    "GRanD_join['StorSource'] = 'NID'\n",
    "\n",
    "#Replace any storage values from the dam attribute file\n",
    "GRanD_join.loc[GRanD_join.Capacity_m3 != 0, 'StorSource'] = GRanD_join.DataSource\n",
    "GRanD_join.loc[GRanD_join.Capacity_m3 != 0, 'MaxStor_m3'] = GRanD_join.Capacity_m3 #dam attribute file and iCold\n",
    "\n",
    "#Replace any with MaxStor == 0 with GRanD storage, removed dams storage, then GDAT storage\n",
    "GRanD_join.loc[GRanD_join.MaxStor_m3 == 0, 'StorSource'] = 'GRanD'\n",
    "GRanD_join.loc[GRanD_join.MaxStor_m3 == 0, 'MaxStor_m3'] = GRanD_join.GRanDCapm3 #GRanD\n",
    "\n",
    "\n",
    "GRanD_join.loc[GRanD_join.MaxStor_m3 == 0, 'StorSource'] = GRanD_join.Batch_for\n",
    "GRanD_join.loc[GRanD_join.MaxStor_m3 == 0, 'MaxStor_m3'] = GRanD_join.OrigCap_m3 #Removed dams\n",
    "\n",
    "\n",
    "GRanD_join.loc[GRanD_join.MaxStor_m3 == 0, 'StorSource'] = 'GDAT'\n",
    "GRanD_join.loc[GRanD_join.MaxStor_m3 == 0, 'MaxStor_m3'] = GRanD_join.Volume_Max #GDAT\n",
    "\n",
    "\n",
    "#Any dams with max-storage = 0 is removed\n",
    "GRanD_join = GRanD_join.drop(index=GRanD_join.loc[GRanD_join.MaxStor_m3 == 0].loc[GRanD_join.IsRiverMth==0].index)\n",
    "\n",
    "\n",
    "print('Size after storage filtering:',GRanD_join.shape)\n",
    "print('Number of sites in database:', GRanD_join.loc[GRanD_join.IsSite == 1].shape)\n",
    "print('Number of Reclamation dams in database:', GRanD_join.loc[GRanD_join.IsUSBR == 1].shape)\n",
    "print('Number of USACE dams in database:', GRanD_join.loc[GRanD_join.IsUSACE == 1].shape)\n",
    "print('Number of GRanD in database:', GRanD_join.loc[GRanD_join.IsGRanD == 1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d97e93",
   "metadata": {},
   "source": [
    "#### Assign ShortIDs to any dam that doesn't already have one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de33b848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign unique ShortID to non-sites\n",
    "\n",
    "# First, sort by ascending NID\n",
    "GRanD_join = GRanD_join.sort_values('NID', ascending = True)\n",
    "\n",
    "# Starting ShortID should be the maximum of the site/GRanD ShortIDs plus 1,000 and rounded to the nearest thousandth\n",
    "startID = math.floor((GRanD_join.ShortID.max() + 1000)/1000)*1000\n",
    "ID = startID\n",
    "\n",
    "# Assign a ShortID to anything that doesn't have one yet       \n",
    "for index, row in GRanD_join.iterrows():\n",
    "    if pd.isna(row['ShortID']):  # Check if ShortID is null\n",
    "        GRanD_join.loc[index, 'ShortID'] = ID\n",
    "        ID += 1  # Increment ID for next ShortID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a074e8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no duplicate ShortIDs\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate ShortIDs. If there are duplicates, you have an error in your site input files. This is because either\n",
    "# you assigned two dams with different NIDs the same ShortID or could have a typo in the NID field.\n",
    "\n",
    "test = GRanD_join.ShortID\n",
    "\n",
    "nodup = set(test)\n",
    "\n",
    "if len(nodup) != len(test):\n",
    "    print('There are duplicate ShortIDs!')\n",
    "\n",
    "    newlist = [] # Empty list to hold unique elements from the list.\n",
    "    duplist = [] # Empty list to hold the duplicate elements from the list.\n",
    "    for i in test:\n",
    "        if i not in newlist:\n",
    "            newlist.append(i)\n",
    "        else:\n",
    "            duplist.append(i) # This method catches the first duplicate entries, and appends them to the list.\n",
    "            \n",
    "    # The next step is to print the duplicate entries, and the unique entries\n",
    "    print(\"List of duplicates\", duplist)\n",
    "else:\n",
    "    print('There are no duplicate ShortIDs')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cc6447",
   "metadata": {},
   "source": [
    "#### Clean up columns and export as a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d63b98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine columns where necessary and drop unwanted columns\n",
    "\n",
    "# Combine NID Dam height and removed dam heights into one field. Only put in removed dam where NID is null\n",
    "GRanD_join['DamH_ft'] = GRanD_join.NID_Height\n",
    "GRanD_join.loc[(GRanD_join.DamH_ft.isnull() == True),'DamH_ft'] = GRanD_join.DAmHft #removed dam database\n",
    "GRanD_join.loc[(GRanD_join.DamH_ft.isnull())==True,'DamH_ft'] = GRanD_join.DAM_HGT_M*3.28 #GRanD\n",
    "GRanD_join.loc[(GRanD_join.DamH_ft.isnull())==True,'DamH_ft'] = GRanD_join.Height*3.28 #GDAT\n",
    "\n",
    "# Fill in Dam function from removed dam file everywhere PrimPurp is null\n",
    "# Then replace the null PrimaryPurp with Purp because some have null PrimaryPurp and non-null Purp.\n",
    "GRanD_join.loc[(GRanD_join.PrimaryPurp.isnull() == True),'PrimaryPurp'] = GRanD_join.DamFunctio\n",
    "GRanD_join.loc[(GRanD_join.PrimaryPurp.isnull() == True),'PrimaryPurp'] = GRanD_join.Purp\n",
    "\n",
    "# Year completed\n",
    "GRanD_join.loc[GRanD_join['yrc'].notna(),'yrc_source'] = GRanD_join.Batch_for #Removed dams\n",
    "GRanD_join['yrc'] = GRanD_join['yrc'].fillna(GRanD_join['Year_Compl']) # Anywhere removed dam database is null, change to NID\n",
    "GRanD_join.loc[(GRanD_join['yrc'].notna()) & (GRanD_join['yrc_source'].isna()),'yrc_source'] = 'NID'\n",
    "GRanD_join.loc[(GRanD_join.Moved == 1) & (GRanD_join.DataSource != 'iCOLD'), 'yrc'] = GRanD_join.Year_Compl_user #Anyting else still null fill from dam attributes file\n",
    "GRanD_join.loc[(GRanD_join.Moved == 1) & (GRanD_join.DataSource != 'iCOLD'), 'yrc_source'] = GRanD_join.DataSource\n",
    "GRanD_join.loc[GRanD_join['yrc'].isna(),'yrc'] = GRanD_join['YEAR'] # Anything else still null fill with GRanD\n",
    "GRanD_join.loc[(GRanD_join['yrc'].notna()) & (GRanD_join['yrc_source'].isna()),'yrc_source'] = 'GRanD'\n",
    "GRanD_join.loc[GRanD_join['yrc'].isna(),'yrc'] = GRanD_join['Year_Compl_user']# Anything else still null fill with iCOLD\n",
    "GRanD_join.loc[(GRanD_join['yrc'].notna()) & (GRanD_join['yrc_source'].isna()),'yrc_source'] = 'iCOLD'\n",
    "GRanD_join.loc[GRanD_join['yrc'].isna(),'yrc'] = GRanD_join['Year_Fin']# Anything else still null fill with GDAT\n",
    "GRanD_join.loc[(GRanD_join['yrc'].notna()) & (GRanD_join['yrc_source'].isna()),'yrc_source'] = 'GDAT'\n",
    "\n",
    "GRanD_join.loc[GRanD_join.yrc.isnull(),'yrc_source'] = np.nan #anywhere with no year completed, fill the source as NaN\n",
    "\n",
    "# Year removed\n",
    "GRanD_join['yrr'] = GRanD_join['yrr'].fillna(0) #Deal with inconsistencies in datatypes in fields\n",
    "GRanD_join['yrr'] = GRanD_join['yrr'].replace(-99,0)\n",
    "GRanD_join['yrr'] = GRanD_join['yrr'].replace(-999,0)\n",
    "GRanD_join['yrr'] = GRanD_join.apply(lambda row: row['YrRemoved'] if row['yrr'] == 0 else row['yrr'], axis=1) #attribute file/USACE/USBR\n",
    "\n",
    "GRanD_join['yrr'] = GRanD_join['yrr'].fillna(0)\n",
    "GRanD_join['yrr'] = GRanD_join['yrr'].replace(-99,0)\n",
    "GRanD_join['yrr'] = GRanD_join['yrr'].replace(-999,0)\n",
    "GRanD_join['yrr'] = GRanD_join.apply(lambda row: row['REM_YEAR'] if row['yrr'] == 0 else row['yrr'], axis=1) #GRanD\n",
    "GRanD_join['yrr'] = GRanD_join['yrr'].fillna(0)\n",
    "GRanD_join['yrr'] = GRanD_join['yrr'].replace(-99,0)\n",
    "GRanD_join['yrr'] = GRanD_join['yrr'].replace(-999,0)\n",
    "\n",
    "# Fill nans with 0\n",
    "GRanD_join['IsRiverMth'] = GRanD_join['IsRiverMth'].fillna(0) #River indicator\n",
    "GRanD_join['delta'] = GRanD_join['delta'].fillna(0) #delta indicator\n",
    "\n",
    "\n",
    "# Dam name\n",
    "GRanD_join.loc[GRanD_join.Dam_Name=='','Dam_Name'] = np.nan # Set values we made blank earlier back to nan\n",
    "GRanD_join.loc[GRanD_join.Dam_Name.isnull()==True, 'Dam_Name'] = GRanD_join.Other_Dam # Replace missing NID names with NID other name first\n",
    "GRanD_join.loc[GRanD_join.Dam_Name.isnull()==True, 'Dam_Name'] = GRanD_join.Reservoir # Then site\n",
    "GRanD_join.loc[GRanD_join.Dam_Name.isnull()==True, 'Dam_Name'] = GRanD_join.RES_NAME # Then GRanD\n",
    "GRanD_join.loc[GRanD_join.Dam_Name.isnull()==True, 'Dam_Name'] = GRanD_join.DamName # Then Removed file\n",
    "\n",
    "\n",
    "# Drop unwanted columns at this point\n",
    "GRanD_join = GRanD_join.drop(['index','Other_Dam','OtherStructureID','FederalID','Longitude','Latitude','River',\n",
    "                              'Owner_Name', 'Max_Storag','Normal_Sto','Lat','Long','ShortID_NID',\n",
    "                              'IsSite_NID','IsUSBR_NID','IsUSACE_NID','DAM_NAME','ALT_NAME',\n",
    "                              'ADMIN_UNIT','LONG_DD','LAT_DD','NIDnotes','ShortID_GRanD','HasNHD',\n",
    "                              'IsSite_GRanD','IsUSBR_GRanD','IsUSACE_GRanD','LAT_GRAND','LONG_GRAND',\n",
    "                              'CitationID','CitationUR','DamAccessi','DamRiverNa','DamRiver_1','DamLocatio',\n",
    "                              'DamState_P','DamLatitud','DamLongitu','DamAccurac','DamOwner',\n",
    "                              'LAT_Rem','LONG_Rem','LAT_Geo','LONG_Geo','NID_Height','DamH_m', 'lat','lon','NewX','NewY',\n",
    "                              'DAmHft','Purp','DamFunctio','Other_Dam','REM_YEAR','RES_NAME',\n",
    "                              'DamName','DAM_HGT_M','YEAR', 'CAP_MCM','method','Year_Fin','Height','Volume_Max',\n",
    "                              'OID__join', 'OID__rem'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6de26876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Year Completed fields to fill in no Year Completed data into yrc.\n",
    "\n",
    "# first, re-index\n",
    "GRanD_join = GRanD_join.reset_index(drop=True)\n",
    "\n",
    "# If yrc is outside of 1700-2023, make it 0 because is likely wrong.\n",
    "GRanD_join.loc[(GRanD_join.yrc < 1700) | (GRanD_join.yrc > 2023), 'yrc'] = 0\n",
    "\n",
    "# If yrc = 0 and Year_Modif from NID ~=0, set yrc to the minimum of Year_Modif\n",
    "for i in range(len(GRanD_join['Year_Modif'])):\n",
    "    years = GRanD_join['Year_Modif'][i]\n",
    "    \n",
    "    if isinstance(years,float):\n",
    "        if np.isnan(years):\n",
    "            years = []\n",
    "    else:\n",
    "        years = str(years)\n",
    "        years = years.split(';')\n",
    "        years = [int(re.search(r'\\d+',year).group()) for year in years]\n",
    "        \n",
    "        if len(years) > 0:\n",
    "            minyr = min(years)\n",
    "            if GRanD_join['yrc'][i] == 0:\n",
    "                GRanD_join.loc[i, 'yrc'] = minyr\n",
    "\n",
    "GRanD_join.loc[GRanD_join.yrc == -99, 'yrc'] = 0\n",
    "GRanD_join.loc[GRanD_join.yrc.isna(),'yrc'] = 0\n",
    "\n",
    "GRanD_join = GRanD_join.drop(['Year_Compl','Year_Modif','Year_Compl_user','YrRemoved'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e017d0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert everything to metric and drop the ft column\n",
    "GRanD_join['Dam_Len_m'] = GRanD_join.Dam_Length*0.3048\n",
    "GRanD_join['SA_m2'] = GRanD_join.Surface_Ar*4046.85642\n",
    "GRanD_join['DA_km2'] = GRanD_join.Drainage_A*2.58998811\n",
    "GRanD_join['MaxQ_m3s'] = GRanD_join.Max_Discha*0.028316847\n",
    "GRanD_join['DamH_m'] = GRanD_join.DamH_ft * 0.3048\n",
    "\n",
    "# Fill in nans in Moved field\n",
    "GRanD_join['Moved'] = GRanD_join['Moved'].fillna(0)\n",
    "\n",
    "# Drop columns with imperial units\n",
    "GRanD_join = GRanD_join.drop(['Dam_Length','Surface_Ar','Drainage_A','Max_Discha',\n",
    "                             'DamNameAlt','DamH_ft','NID_Storag'\n",
    "                              ],axis=1)\n",
    "# Rename columns\n",
    "GRanD_join.rename(columns = {'OrigCap_m3':'OCapm3_Rem','LAT_JOIN':'LAT_FINAL','LONG_JOIN':'LONG_FINAL',\n",
    "                             'DA_km':'site_DA_km'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e506bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export final files as a csv\n",
    "GRanD.to_csv(os.path.join(out_folder,'GRanD.csv'))\n",
    "GRanD_join.to_csv(os.path.join(out_folder,'NID_GRanDjoin.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b01ac01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Size moving to snapping:',GRanD_join.shape)\n",
    "print('Number of sites in database:', GRanD_join.loc[GRanD_join.IsSite == 1].shape)\n",
    "print('Number of Reclamation dams in database:', GRanD_join.loc[GRanD_join.IsUSBR == 1].shape)\n",
    "print('Number of USACE dams in database:', GRanD_join.loc[GRanD_join.IsUSACE == 1].shape)\n",
    "print('Number of GRanD in database:', GRanD_join.loc[GRanD_join.IsGRanD == 1].shape)\n",
    "print('Number of river mouths in database:',GRanD_join.loc[GRanD_join.IsRiverMth==1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b74d07",
   "metadata": {},
   "source": [
    "# 2. Snap dams to NHDPlus Flowlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e590628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='gpresult'><h2>Messages</h2><div id='messages' data-messages='[\"Start Time: Thursday, November 7, 2024 5:56:32 PM\",\"NID_filtered Successfully converted:  E:/ResSed/MediumResolution_DamLinkages/Manuscript/OutputsDamNet1107\\\\NID_filtered.shp\",\"Succeeded at Thursday, November 7, 2024 5:57:05 PM (Elapsed Time: 32.83 seconds)\"]' data-show='true'><div id = 'default' /></div></div>"
      ],
      "text/plain": [
       "<Result 'E:\\\\ResSed\\\\MediumResolution_DamLinkages\\\\Manuscript\\\\OutputsDamNet1107'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Snap dams to NHDPlus HR Flowlines: must be done with arcPy. Ensure that all layers are in the same coordinate system (here we use NAD83).\n",
    "\n",
    "# First convert the csv to a shapefile\n",
    "XFieldName = 'LONG_FINAL'\n",
    "YFieldName = 'LAT_FINAL'\n",
    "newLayerName = \"NID_filtered\" # Name of your output shapefile\n",
    "\n",
    "spatialRef = arcpy.SpatialReference(4269) # Spatial reference WKID for NAD83\n",
    "csvFilePath = os.path.join(out_folder,'NID_GRanDjoin.csv') # Your filtered dam dataset csv\n",
    "\n",
    "\n",
    "\n",
    "arcpy.MakeXYEventLayer_management(csvFilePath, XFieldName, YFieldName, newLayerName, spatial_reference=spatialRef)\n",
    "arcpy.FeatureClassToShapefile_conversion(newLayerName, out_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6c7a94-123d-41c8-8aaa-9846afb7ab18",
   "metadata": {},
   "source": [
    "*The following 2 cells will take multiple hours to run, so plan accordingly.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0fd104d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total non-site initial selection: 86359\n",
      "Total snapped to flowline: 30961\n",
      "Total moving to storage >= 5e6 m$^3$ step: 1416\n",
      "Total snapped to flowline: 814\n",
      "Total moving to storage < 5e6 m$^3$ step: 53982\n",
      "Total snapped to flowline: 24930\n",
      "Total sites snapped to flowline: 708\n",
      "Total GRanD snapped to flowline: 1791\n",
      "Total USBR snapped to flowline: 257\n",
      "Total USACE snapped to flowline: 451\n",
      "Total moved dams snapped to flowline: 2201\n",
      "Total dams that do not snap: 29787\n",
      "Total dams in DamNet: 60043\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class='gpresult'><h2>Messages</h2><div id='messages' data-messages='[\"Start Time: Thursday, November 7, 2024 8:08:09 PM\",\"Succeeded at Thursday, November 7, 2024 8:08:11 PM (Elapsed Time: 1.88 seconds)\"]' data-show='true'><div id = 'default' /></div></div>"
      ],
      "text/plain": [
       "<Result 'NIDFiltered_snap'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For NHDPlus\n",
    "NIDFiltered = os.path.join(out_folder,'NID_filtered.shp') #Link to your filtered NID shapefile\n",
    "\n",
    "NIDlyr = \"NIDlyr\" #create a layer file\n",
    "NHDlyr = \"NHDlyr\"\n",
    "arcpy.management.MakeFeatureLayer(NIDFiltered,NIDlyr) #convert the feature class to a layer to work from\n",
    "arcpy.management.MakeFeatureLayer(NHDFlowline,NHDlyr)\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "# Run the near tool to get the new lat/long with near FType558\n",
    "arcpy.management.SelectLayerByAttribute(NHDlyr,'NEW_SELECTION',\"FCODE = 55800\")\n",
    "arcpy.management.SelectLayerByAttribute(NHDlyr,'SUBSET_SELECTION',\"DivDASqKM > 0\") #only select flowlines with > 0 sq.km.\n",
    "\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'NEW_SELECTION',\"IsSite = 0\") #select all non-sites, non-Reclamation, non-USACE, and non-GRanD to snap\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"IsGRanD = 0\")\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"IsUSBR = 0\")\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"IsUSACE = 0\")\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"Moved = 0\")\n",
    "\n",
    "# Print counts\n",
    "print('Total non-site initial selection:',arcpy.management.GetCount(NIDlyr))\n",
    "\n",
    "# Near 250 m to FType558 for everything that isn't a site\n",
    "arcpy.analysis.Near(NIDlyr,NHDlyr,\"250 Meters\",\"LOCATION\",\"NO_ANGLE\",\"PLANAR\")\n",
    "\n",
    "# Create a new field and populate it\n",
    "arcpy.management.AddField(NIDlyr,'NrX_Final',\"DOUBLE\")\n",
    "arcpy.management.AddField(NIDlyr,'NrY_Final',\"DOUBLE\")\n",
    "\n",
    "\n",
    "arcpy.management.SelectLayerByAttribute(NHDlyr,'CLEAR_SELECTION') #clear the flowline FType 558 Selection before transferring over values\n",
    "\n",
    "# Transfer values over\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrX_Final\",\"!NEAR_X!\", \"PYTHON3\")\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrY_Final\",\"!NEAR_Y!\", \"PYTHON3\")\n",
    "\n",
    "arcpy.management.DeleteField(NIDlyr,['NEAR_X','NEAR_Y','NEAR_DIST','NEAR_FID'])\n",
    "\n",
    "# Print counts\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"NrX_Final <> -1\")\n",
    "print('Total snapped to flowline:', arcpy.management.GetCount(NIDlyr))\n",
    "\n",
    "\n",
    "#########################################################################################\n",
    "# Near 500m to all flowlines for any that didn't snap and have MaxStor >= 4000 AF/5,0000,000 m^3\n",
    "arcpy.management.SelectLayerByAttribute(NHDlyr,'NEW_SELECTION',\"FCODE <> 56600\") #select NHD that isn't a coastline\n",
    "arcpy.management.SelectLayerByAttribute(NHDlyr,'SUBSET_SELECTION',\"DivDASqKM > 0\") #only select flowlines with > 0 sq.km.\n",
    "\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'NEW_SELECTION',\"NrX_Final = -1\") #Select any Dams that didn't snap\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"IsSite = 0\") #And any that aren't sites\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"IsGRanD = 0\") #And not GRanD\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"IsUSBR = 0\") #And not Reclamation\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"IsUSACE = 0\") #And not Army Corps\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"Moved = 0\")\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"MaxStor_m3>=5e6\") #MaxStor >= 4000 AF\n",
    "\n",
    "# Print counts\n",
    "print('Total moving to storage >= 5e6 m$^3$ step:', arcpy.management.GetCount(NIDlyr))\n",
    "\n",
    "arcpy.analysis.Near(NIDlyr,NHDlyr,\"500 Meters\",\"LOCATION\",\"NO_ANGLE\",\"PLANAR\")\n",
    "\n",
    "# Transfer values over\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrX_Final\",\"!NEAR_X!\", \"PYTHON3\")\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrY_Final\",\"!NEAR_Y!\", \"PYTHON3\")\n",
    "\n",
    "arcpy.management.DeleteField(NIDlyr,['NEAR_X','NEAR_Y','NEAR_DIST','NEAR_FID'])\n",
    "\n",
    "arcpy.management.SelectLayerByAttribute(NHDlyr,'CLEAR_SELECTION') #clear the flowline Selection before transferring over values\n",
    "\n",
    "# Print counts\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"NrX_Final <> -1\")\n",
    "print('Total snapped to flowline:', arcpy.management.GetCount(NIDlyr))\n",
    "\n",
    "\n",
    "#######################################################################################\n",
    "# Near 250m to all flowlines for any that didn't snap and still aren't a site\n",
    "arcpy.management.SelectLayerByAttribute(NHDlyr,'NEW_SELECTION',\"FCODE <> 56600\") #select NHD that isn't a coastline\n",
    "arcpy.management.SelectLayerByAttribute(NHDlyr,'SUBSET_SELECTION',\"DivDASqKM > 0\") #only select flowlines with > 0 sq.km.\n",
    "\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'NEW_SELECTION',\"NrX_Final = -1\") #Select any Dams that didn't snap\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"IsSite = 0\") #And any that aren't sites\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"IsGRanD = 0\") #And any that aren't GRanD\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"IsUSBR = 0\") #And not Reclamation\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"IsUSACE = 0\") #And not Army Corps\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"Moved = 0\")\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"MaxStor_m3<5e6\") #MaxStor < 4000 AF\n",
    "\n",
    "\n",
    "# Print counts\n",
    "print('Total moving to storage < 5e6 m$^3$ step:', arcpy.management.GetCount(NIDlyr))\n",
    "\n",
    "arcpy.analysis.Near(NIDlyr,NHDlyr,\"250 Meters\",\"LOCATION\",\"NO_ANGLE\",\"PLANAR\")\n",
    "\n",
    "# Transfer values over\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrX_Final\",\"!NEAR_X!\", \"PYTHON3\")\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrY_Final\",\"!NEAR_Y!\", \"PYTHON3\")\n",
    "\n",
    "arcpy.management.DeleteField(NIDlyr,['NEAR_X','NEAR_Y','NEAR_DIST','NEAR_FID'])\n",
    "\n",
    "arcpy.management.SelectLayerByAttribute(NHDlyr,'CLEAR_SELECTION') #clear the flowline Selection before transferring over values\n",
    "\n",
    "# Print counts\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"NrX_Final <> -1\")\n",
    "print('Total snapped to flowline:', arcpy.management.GetCount(NIDlyr))\n",
    "\n",
    "\n",
    "######################################################################################\n",
    "# Sites to nearest flowline using near 250m b/c should be in the correct place.\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'CLEAR_SELECTION') #clear the NID selection\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'NEW_SELECTION',\"IsSite = 1\") #Select all sites\n",
    "\n",
    "arcpy.analysis.Near(NIDlyr,NHDlyr,\"250 Meters\",\"LOCATION\",\"NO_ANGLE\",\"PLANAR\")\n",
    "\n",
    "# Transfer values over\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrX_Final\",\"!NEAR_X!\", \"PYTHON3\")\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrY_Final\",\"!NEAR_Y!\", \"PYTHON3\")\n",
    "\n",
    "arcpy.management.DeleteField(NIDlyr,['NEAR_X','NEAR_Y','NEAR_DIST','NEAR_FID'])\n",
    "\n",
    "\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"NrX_Final <> -1\")\n",
    "print('Total sites snapped to flowline:', arcpy.management.GetCount(NIDlyr))\n",
    "\n",
    "\n",
    "# GRanD to nearest flowline using near 250m b/c should be in the correct place.\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'CLEAR_SELECTION') #clear the NID selection\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'NEW_SELECTION',\"IsGRanD = 1\") #Select all GRanD\n",
    "\n",
    "arcpy.analysis.Near(NIDlyr,NHDlyr,\"250 Meters\",\"LOCATION\",\"NO_ANGLE\",\"PLANAR\")\n",
    "\n",
    "# Transfer values over\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrX_Final\",\"!NEAR_X!\", \"PYTHON3\")\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrY_Final\",\"!NEAR_Y!\", \"PYTHON3\")\n",
    "\n",
    "arcpy.management.DeleteField(NIDlyr,['NEAR_X','NEAR_Y','NEAR_DIST','NEAR_FID'])\n",
    "\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"NrX_Final <> -1\")\n",
    "print('Total GRanD snapped to flowline:', arcpy.management.GetCount(NIDlyr))\n",
    "\n",
    "# USBR to nearest flowline using near 250m b/c should be in the correct place.\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'CLEAR_SELECTION') #clear the NID selection\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'NEW_SELECTION',\"IsUSBR = 1\") #Select all USBR\n",
    "\n",
    "arcpy.analysis.Near(NIDlyr,NHDlyr,\"250 Meters\",\"LOCATION\",\"NO_ANGLE\",\"PLANAR\")\n",
    "\n",
    "# Transfer values over\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrX_Final\",\"!NEAR_X!\", \"PYTHON3\")\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrY_Final\",\"!NEAR_Y!\", \"PYTHON3\")\n",
    "\n",
    "arcpy.management.DeleteField(NIDlyr,['NEAR_X','NEAR_Y','NEAR_DIST','NEAR_FID'])\n",
    "\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"NrX_Final <> -1\")\n",
    "print('Total USBR snapped to flowline:', arcpy.management.GetCount(NIDlyr))\n",
    "\n",
    "\n",
    "# USACE to nearest flowline using near 250m b/c should be in the correct place.\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'CLEAR_SELECTION') #clear the NID selection\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'NEW_SELECTION',\"IsUSACE = 1\") #Select all USACE\n",
    "\n",
    "arcpy.analysis.Near(NIDlyr,NHDlyr,\"250 Meters\",\"LOCATION\",\"NO_ANGLE\",\"PLANAR\")\n",
    "\n",
    "# Transfer values over\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrX_Final\",\"!NEAR_X!\", \"PYTHON3\")\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrY_Final\",\"!NEAR_Y!\", \"PYTHON3\")\n",
    "\n",
    "arcpy.management.DeleteField(NIDlyr,['NEAR_X','NEAR_Y','NEAR_DIST','NEAR_FID'])\n",
    "\n",
    "\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"NrX_Final <> -1\")\n",
    "print('Total USACE snapped to flowline:', arcpy.management.GetCount(NIDlyr))\n",
    "\n",
    "# Moved dams to nearest flowline using near 250m b/c should be in the correct place.\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'CLEAR_SELECTION') #clear the NID selection\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'NEW_SELECTION',\"Moved = 1\") #Select all dams moved in the NID edit file\n",
    "\n",
    "arcpy.analysis.Near(NIDlyr,NHDlyr,\"250 Meters\",\"LOCATION\",\"NO_ANGLE\",\"PLANAR\")\n",
    "\n",
    "# Transfer values over\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrX_Final\",\"!NEAR_X!\", \"PYTHON3\")\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrY_Final\",\"!NEAR_Y!\", \"PYTHON3\")\n",
    "\n",
    "arcpy.management.DeleteField(NIDlyr,['NEAR_X','NEAR_Y','NEAR_DIST','NEAR_FID'])\n",
    "\n",
    "\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"NrX_Final <> -1\")\n",
    "print('Total moved dams snapped to flowline:', arcpy.management.GetCount(NIDlyr))\n",
    "\n",
    "\n",
    "# Delete any features that did not snap.\n",
    "# First select non-snaps\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'CLEAR_SELECTION')\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'NEW_SELECTION',\"NrX_Final = -1\")\n",
    "\n",
    "print('Total dams that do not snap:', arcpy.management.GetCount(NIDlyr))\n",
    "\n",
    "# Delete the selected rows:\n",
    "arcpy.management.DeleteRows(NIDlyr)\n",
    "\n",
    "print('Total dams in ResNet:', arcpy.management.GetCount(NIDlyr))\n",
    "\n",
    "\n",
    "# Display XY data using NearX and NearY to move the points onto the NHD Flowlines\n",
    "arcpy.management.MakeXYEventLayer(NIDlyr, \"NrX_Final\", \"NrY_Final\", 'NIDFiltered_snap', spatialRef)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9885d771",
   "metadata": {},
   "source": [
    "### Intersect with the flowline data to extract attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09de94ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intersect file with moved dams to the flowline file:\n",
    "arcpy.analysis.Intersect(['NIDFiltered_snap',NHDFlowline],os.path.join(out_folder,'NIDFiltered_snap.shp'),\"ALL\",None,\"INPUT\")\n",
    "\n",
    "# Export NID file\n",
    "dbf = Dbf5(os.path.join(out_folder,'NIDFiltered_snap.dbf'))\n",
    "df = dbf.to_dataframe()\n",
    "df.to_csv(os.path.join(out_folder,'NID_filtered_snapped.csv'),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4775fbd",
   "metadata": {},
   "source": [
    "### Remove dams snapping to duplicate flowlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "deac67b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size before duplicates removed: (76196, 179)\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate flowlines. When you run intersect in ArcGIS, any intersects that happen at the join of two lines gives two\n",
    "# results in the final table. We need to delete one of these.\n",
    "\n",
    "NID = pd.read_csv(os.path.join(out_folder,'NID_filtered_snapped.csv'), low_memory=False) # Load data.\n",
    "\n",
    "# Delete any that snapped to no-drainage area flowlines that aren't sites\n",
    "NID = NID.drop(index=NID.loc[NID.DivDASqKM == 0].index)\n",
    "\n",
    "NID_sort = NID.sort_values('ShortID', ascending = True) # Sort by ascending ShortID.\n",
    "\n",
    "\n",
    "print('Size before duplicates removed:', NID_sort.shape)\n",
    "\n",
    "# Convert dataframe to dictionaries (struct-like); basically has format column->value.\n",
    "NID_dict = NID_sort.to_dict(orient = 'records')\n",
    "dupl_ordered_dict = NID.to_dict(orient='records')\n",
    "\n",
    "# Initialize empty list to store indices of non-duplicates.\n",
    "dupind = []\n",
    "\n",
    "# Identify unique values and their counts.\n",
    "shortID = [item['ShortID'] for item in dupl_ordered_dict]\n",
    "uniquevals,ia = np.unique(shortID, return_inverse = True)\n",
    "\n",
    "# Count the frequency of each index in ia.\n",
    "bincounts = np.bincount(ia)\n",
    "\n",
    "# Zero out singles.\n",
    "singles = uniquevals[bincounts <= 1]\n",
    "singleidx = [i for i, val in enumerate(shortID) if val in singles]\n",
    "for idx in singleidx:\n",
    "    shortID[idx] = 0\n",
    "    \n",
    "# Overwrite repeats.\n",
    "repeats = uniquevals[bincounts > 1]\n",
    "shortID = np.array([np.where(repeats == val)[0][0] + 1 if val in repeats else val for val in shortID])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd3f9f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size after removing duplicates: (60033, 179)\n",
      "Number of sites in database: (708, 179)\n",
      "Number of Reclamation dams in database: (257, 179)\n",
      "Number of USACE dams in database: (451, 179)\n",
      "Number of GRanD in database: (1790, 179)\n",
      "Number of river mouths in database: (143, 179)\n"
     ]
    }
   ],
   "source": [
    "skip_it = 0; # Initialize a counter.\n",
    "# Pull out FCODE and Hydrosequence fields to help with decision tree for removing duplicates.\n",
    "FCODE = [item['FCODE'] for item in dupl_ordered_dict]\n",
    "Hydroseq = [item['Hydroseq'] for item in dupl_ordered_dict]\n",
    "\n",
    "\n",
    "for i in range(len(shortID)):\n",
    "    if shortID[i] == 0: # If it is not a duplicate, keep it.\n",
    "        dupind.append(i)\n",
    "    elif skip_it > 0: # Or if we already dealt with it, update the counter so it gets skipped.\n",
    "        skip_it -= 1\n",
    "        continue\n",
    "    else: # Else the value is a duplicate.\n",
    "        dup = [idx for idx, val in enumerate(shortID) if val == shortID[i]] # Gives all indices of the duplicates.\n",
    "        dup1 = dup[0]\n",
    "        dupskip = dup1 # Keep track of what the first index was because we will change this.\n",
    "        j = len(dup)\n",
    "        jskip = j # Same for the length of the duplicates.\n",
    "\n",
    "        Hydro = Hydroseq[dup1:dup[j-1]+1] # Pull out Hydrosequences as the duplicates.\n",
    "\n",
    "        kept_indices = [i for i, x in enumerate(Hydro) if x not in [Hydroseq[i] for i in dupind]] # If a dam is already snapped to that flowline, remove the flowlines from the options to choose from.\n",
    "\n",
    "        dup_test = [dup[i] for i in kept_indices]\n",
    "                \n",
    "        if len(dup_test) == 0: # All of the flowline options have already been used, in which case just keep them all. Duplicate snaps are removed later.\n",
    "            dup_test = dup\n",
    "        \n",
    "        dup = dup_test\n",
    "        \n",
    "        dup1 = dup[0]\n",
    "        j = len(dup)\n",
    "        \n",
    "        Floc = [index for index, value in enumerate(FCODE[dup1:dup[j-1]+1]) if value == 55800] # Pull out FCODE = 55800 for duplicates (flowlines in reservoirs).\n",
    "        coast = [index for index, value in enumerate(FCODE[dup1:dup[j-1]+1]) if value == 56600] # Pull out any duplicates that are on a coast flowline.\n",
    "           \n",
    "        if len(Floc) == 1: # If only one value is FType 558.\n",
    "            dupind.append(dup[Floc[0]])\n",
    "        elif len(Floc) == j: # All of the values are 558, take smallest hydroseq (most downstream).\n",
    "            Hydro = Hydroseq[dup1:dup[j-1]+1]\n",
    "            minloc = np.argmin(Hydro)\n",
    "            dupind.append(dup[minloc])\n",
    "        elif len(Floc) == 0: # None are FType 558.         \n",
    "            Hydro = Hydroseq[dup1:dup[j-1]+1]\n",
    "           \n",
    "            if len(coast) > 0:\n",
    "                Hydro = np.delete(Hydro,coast) # Remove coastal values; any dam that snaps to a coast flowline has it's dam order messed up and can route along the coast.\n",
    "                dup = np.delete(dup,coast)\n",
    "\n",
    "            minloc = np.argmin(Hydro) # Currently taking minimum of the new hydro.\n",
    "            dupind.append(dup[minloc])\n",
    "        else: # Some other number of values is FType 558; still take the most downstream.\n",
    "            Hydro = [Hydroseq[dup[index]] for index in Floc]\n",
    "    \n",
    "            if len(coast) > 0:\n",
    "                Hydro = np.delete(Hydro,coast)\n",
    "                dup = np.delete(dup,coast)\n",
    "                \n",
    "            minloc = np.argmin(Hydro)\n",
    "            dupind.append(dup[Floc[minloc]])\n",
    "            \n",
    "        if dupskip == i: # If the first index was the current index.\n",
    "            skip_it = jskip-1 # Skip the next j-1 indices.\n",
    "        else:\n",
    "            skip_it = 0\n",
    "\n",
    "dupltable = pd.DataFrame.from_dict(dupl_ordered_dict)\n",
    "noduplicates = dupltable.loc[dupind]\n",
    "\n",
    "\n",
    "\n",
    "print('Size after removing duplicates:',noduplicates.shape) # New database size after snapping to NHD flowlines\n",
    "print('Number of sites in database:', noduplicates.loc[noduplicates.IsSite == 1].shape)\n",
    "print('Number of Reclamation dams in database:', noduplicates.loc[noduplicates.IsUSBR == 1].shape)\n",
    "print('Number of USACE dams in database:', noduplicates.loc[noduplicates.IsUSACE == 1].shape)\n",
    "print('Number of GRanD in database:', noduplicates.loc[noduplicates.IsGRanD == 1].shape)\n",
    "print('Number of river mouths in database:', noduplicates.loc[noduplicates.IsRiverMth == 1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e1d37f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-order fields and drop new additions we do not want\n",
    "noduplicates = noduplicates[['Dam_Name','ShortID', 'NID', 'GRAND_ID', 'IsSite', 'IsUSBR', 'IsUSACE', 'IsGRanD', 'State', 'OwnerTypes', \n",
    "        'PrimaryPur', 'PrimDamTyp', 'Reservoir',\n",
    "        'IsRiverMth', 'delta', 'IsLock', 'yrc', 'yrr', 'yrc_source',\n",
    "        'MaxStor_m3', 'StorSource', 'Dam_Len_m', \n",
    "        'SA_m2', 'DA_km2', 'MaxQ_m3s', 'DamH_m', 'NrX_Final', 'NrY_Final', 'COMID', 'LENGTHKM', 'FCODE', 'Hydroseq', \n",
    "        'Pathlength', 'DnHydroseq', 'DivDASqKM', 'Country_ou', \n",
    "        'WBCOMID','TerminalPa','Moved']]\n",
    "\n",
    "noduplicates.to_csv(os.path.join(out_folder,'NID_filtered_snapped_nodupl.csv'),index = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ResNet)",
   "language": "python",
   "name": "resnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
