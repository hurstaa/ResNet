{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71cb2a90",
   "metadata": {},
   "source": [
    "#### This workflow was originally written 09/09/2021 by Aaron Hurst to automate ordering dams from upstream to downstream using the NHDPlus Flowline dataset and NID dams that were previously filtered and snapped to the flowlines using the 01DamNet_FilteringAndSnapping iPython notebook. Modifications have been made since then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "310879a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "pd.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81d61eda",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NID' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m outputFile = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33mOutputs/ResNet\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoday\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.csv\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Sort the dataframe by descending Hydrosequence\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m dam = \u001b[43mNID\u001b[49m.sort_values(\u001b[33m'\u001b[39m\u001b[33mHydroseq\u001b[39m\u001b[33m'\u001b[39m, ascending = \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     18\u001b[39m dam = dam.reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'NID' is not defined"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "\n",
    "# Load file with dams. These are NID dams that were filtered with 01DamNet_FilteringAndSnapping and intersected with NHD Flowlines\n",
    "# to get flowline attributes for each dam. This file must contain the COMID and Hydrosequence data. The second file is the\n",
    "# NHDPlus Flowline dataset exported as a csv. This file will be located in your output folder from the filtering and snapping\n",
    "# step and is called 'NID_filtered_snapped_nodupl.csv'.\n",
    "NID = pd.read_csv(\"Outputs/NID_filtered_snapped_nodupl.csv\", low_memory=False)\n",
    "\n",
    "# Load NHDPlus table. This has the option of a CountryOut code that tells which coast the flowline exits at. \n",
    "NHD = pd.read_csv('Inputs/NHD_Flowline_Network_countriesOut.csv', low_memory=False)\n",
    "\n",
    "# Output File location and name\n",
    "today = datetime.today().strftime('%Y%m%d')\n",
    "outputFile = f'Outputs/ResNet_{today}.csv'\n",
    "\n",
    "# Sort the dataframe by descending Hydrosequence\n",
    "dam = NID.sort_values('Hydroseq', ascending = False)\n",
    "dam = dam.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4118abad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If multiple dams snapped to the same flowline, pick one. The order of operations for choosing the correct dam is to first\n",
    "# pick dam attribute dams or GRanD dams. Then pick the one that has a completion year, followed by maximum storage, largest dam height,\n",
    "# and then largest ShortID arbitrarily. If multiple have a completeion year, the code goes through the rest of the prioritization\n",
    "# until there is only one dam left.\n",
    "\n",
    "#Create flags field\n",
    "flags = [None] * len(dam['ShortID'])\n",
    "\n",
    "_, uniqueIdx = np.unique(dam['Hydroseq'], return_index=True)\n",
    "duplicate_indices = np.setdiff1d(np.arange(len(dam['Hydroseq'])), uniqueIdx) # Find the indices of elements that are not the first occurrence (i.e., duplicates)\n",
    "duplicate_values = dam['Hydroseq'].iloc[duplicate_indices] # Get the actual duplicate values based on these indices\n",
    "dupeIdx = np.isin(dam['Hydroseq'], duplicate_values) # Find which elements in dam['Hydroseq'] are among the duplicate values\n",
    "dupes = dam['Hydroseq'][dupeIdx] #contains all including first value\n",
    "dupeLoc = np.where(dupeIdx)[0]\n",
    "\n",
    "for loc in dupeLoc:\n",
    "    flags[loc] = [5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "908cc3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty array for indices of non-duplicates we want to keep\n",
    "dupind = []\n",
    "\n",
    "#convert DF to dictionaries (struct-like); basically has format column->value\n",
    "dupl_ordered_dict = dam.to_dict(orient = 'records')\n",
    "\n",
    "# Identify unique values and their counts\n",
    "Hydro_dupe = [item['Hydroseq'] for item in dupl_ordered_dict]\n",
    "uniquevals,ia = np.unique(Hydro_dupe, return_inverse = True)\n",
    "uniquevals = np.flip(uniquevals)\n",
    "\n",
    "#Count the frequency of each index in ia\n",
    "bincounts = np.bincount(ia)\n",
    "bincounts = np.flip(bincounts)\n",
    "\n",
    "#Zero out non-duplicates\n",
    "singles = uniquevals[bincounts <= 1]\n",
    "singleidx = [i for i, val in enumerate(Hydro_dupe) if val in singles]\n",
    "for idx in singleidx:\n",
    "    Hydro_dupe[idx] = 0\n",
    "    \n",
    "#Overwrite repeats\n",
    "repeats = uniquevals[bincounts > 1]\n",
    "Hydro_dupe = np.array([np.where(repeats==val)[0][0] + 1 if val in repeats else val for val in Hydro_dupe])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c7af3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length after removing duplicate snaps to same hydrosequence: 57454\n"
     ]
    }
   ],
   "source": [
    "skip_it = 0\n",
    "\n",
    "YEAR = [item['yrc'] for item in dupl_ordered_dict]\n",
    "GRAND = [item['IsGRanD'] for item in dupl_ordered_dict]\n",
    "Move = [item['Moved'] for item in dupl_ordered_dict]\n",
    "\n",
    "# Loop through Hydro_dupe values\n",
    "for i in range(len(Hydro_dupe)):\n",
    "    if Hydro_dupe[i] == 0:  # If the value is not a duplicate, keep it\n",
    "        dupind.append(i)\n",
    "        continue \n",
    "        \n",
    "    elif skip_it > 0:  # If we already dealt with an index, skip it\n",
    "        skip_it -= 1\n",
    "        continue\n",
    "\n",
    "    else:  # The value is a duplicate\n",
    "        dup = [idx for idx, val in enumerate(Hydro_dupe) if val == Hydro_dupe[i]]  # Get indices of the duplicates\n",
    "        dup1 = dup[0]\n",
    "        j = len(dup)\n",
    "\n",
    "        skip_it = j - 1\n",
    "\n",
    "        # Pull out variables at the duplicate indices\n",
    "        YearCompl = [index for index, value in enumerate(YEAR[dup1:dup[j-1]+1]) if value > 0]\n",
    "        GRanDloc = [index for index, value in enumerate(GRAND[dup1:dup[j-1]+1]) if value == 1]\n",
    "        moveloc = [index for index, value in enumerate(Move[dup1:dup[j-1]+1]) if value == 1]\n",
    "        \n",
    "        MM = []\n",
    "\n",
    "        if len(GRanDloc) > 0:  # It's a GRanD dam\n",
    "            dupind.append(dup[GRanDloc[0]])\n",
    "            continue\n",
    "        elif len(moveloc) > 0:\n",
    "            dupind.append(dup[moveloc[0]])\n",
    "            continue\n",
    "        elif len(YearCompl) > 0:\n",
    "            if len(YearCompl) < len(dup):  # If all of the dams don't have a completion year, get rid of those\n",
    "                dup = [dup[i] for i in YearCompl]\n",
    "                \n",
    "                # Then take max storage of those\n",
    "                MM = dam['MaxStor_m3'][dup]\n",
    "                \n",
    "                dup = [dup[idx] for idx in (np.where(MM==np.max(MM))[0])]\n",
    "                    \n",
    "                if len(dup) > 1:  # If the storage values are the same, take the largest dam height\n",
    "                    HH = dam['DamH_m'][dup]\n",
    "                    dup = [dup[idx] for idx in (np.where(HH==np.max(HH))[0])]\n",
    "\n",
    "                    if len(dup) > 1: #If the dam heights are the same,  take the oldest dam\n",
    "                        age = dam['yrc'][dup]\n",
    "                        dup = [dup[idx] for idx in (np.where(age == np.min(age))[0])]\n",
    "                        \n",
    "                        if len(dup)>1: #if dam ages are the same, take the largest surface area\n",
    "                            SA = dam['SA_m2'][dup]\n",
    "                            dup = [dup[idx] for idx in (np.where(SA==np.max(SA))[0])]\n",
    "                            \n",
    "                            if len(dup) > 1:  # If the surface areas are the same, take the largest dam length arbitrarily\n",
    "                                length = dam['Dam_Len_m'][dup]\n",
    "                                dup = [dup[idx] for idx in (np.where(length==np.max(length))[0])]\n",
    "                                \n",
    "                                if len(dup)>1: # If dam lengths are the same, take any that aren't privately owned.\n",
    "                                    owner = dam['OwnerTypes'][dup]\n",
    "                                    if (owner == 'Private').all():\n",
    "                                        dup = dup  # Keep all indices if all are 'Private'\n",
    "                                    else:\n",
    "                                        non_private_idx = owner.index[owner != 'Private'].tolist()\n",
    "                                        dup = [idx for idx in dup if idx in non_private_idx]\n",
    "                                    \n",
    "                                    if len(dup)>1:\n",
    "                                        SID = dam['ShortID'][dup]\n",
    "                                        dup = dup[np.where(SID == np.max(SID))[0][0]] #Otherwise take max of ShortID arbitrarily\n",
    "                                    else:\n",
    "                                        dup = dup[0]\n",
    "                                else:\n",
    "                                    dup = dup[0]\n",
    "                            else:\n",
    "                                dup = dup[0]\n",
    "                        else:\n",
    "                            dup = dup[0]\n",
    "                    else:\n",
    "                        dup = dup[0]\n",
    "                else:\n",
    "                    dup = dup[0]\n",
    "\n",
    "                dupind.append(dup)\n",
    "            else:\n",
    "                # Otherwise just take the max storage\n",
    "                MM = dam['MaxStor_m3'][dup]\n",
    "                dup = [dup[idx] for idx in (np.where(MM==np.max(MM))[0])]\n",
    "    \n",
    "                if len(dup) > 1:  # If the storage values are the same, take the largest dam height\n",
    "                    HH = dam['DamH_m'][dup]\n",
    "                    dup = [dup[idx] for idx in (np.where(HH==np.max(HH))[0])]\n",
    "\n",
    "                    if len(dup) > 1: #If the dam heights are the same,  take the oldest dam\n",
    "                        age = dam['yrc'][dup]\n",
    "                        dup = [dup[idx] for idx in (np.where(age == np.min(age))[0])]\n",
    "                        \n",
    "                        if len(dup)>1: # If dam ages are the same, take the largest surface area\n",
    "                            SA = dam['SA_m2'][dup]\n",
    "                            dup = [dup[idx] for idx in (np.where(SA==np.max(SA))[0])]\n",
    "                            \n",
    "                            if len(dup) > 1:  # If the surface areas are the same, take the largest dam length arbitrarily\n",
    "                                length = dam['Dam_Len_m'][dup]\n",
    "                                dup = [dup[idx] for idx in (np.where(length==np.max(length))[0])]\n",
    "                                \n",
    "                                if len(dup)>1: # If dam lengths are the same, take any that aren't privately owned.\n",
    "                                    owner = dam['OwnerTypes'][dup]\n",
    "                                    if (owner == 'Private').all():\n",
    "                                        dup = dup  # Keep all indices if all are 'Private'\n",
    "                                    else:\n",
    "                                        non_private_idx = owner.index[owner != 'Private'].tolist()\n",
    "                                        dup = [idx for idx in dup if idx in non_private_idx]\n",
    "                                    \n",
    "                                    if len(dup)>1:\n",
    "                                        SID = dam['ShortID'][dup]\n",
    "                                        dup = dup[np.where(SID == np.max(SID))[0][0]]\n",
    "                                        \n",
    "                                    else:\n",
    "                                        dup = dup[0]\n",
    "                                else:\n",
    "                                    dup = dup[0]\n",
    "                            else:\n",
    "                                dup = dup[0]\n",
    "                        else:\n",
    "                            dup = dup[0]\n",
    "                    else:\n",
    "                        dup = dup[0]\n",
    "                else:\n",
    "                    dup = dup[0]\n",
    "\n",
    "                dupind.append(dup)\n",
    "\n",
    "            continue\n",
    "        else:\n",
    "            # Otherwise just take the max storage\n",
    "            MM = dam['MaxStor_m3'][dup]\n",
    "            dup = [dup[idx] for idx in (np.where(MM==np.max(MM))[0])]\n",
    "            \n",
    "            if len(dup) > 1:  # If the storage values are the same, take the largest dam height\n",
    "                HH = dam['DamH_m'][dup]\n",
    "                dup = [dup[idx] for idx in (np.where(HH==np.max(HH))[0])]\n",
    "\n",
    "                if len(dup) > 1: #If the dam heights are the same,  take the oldest dam\n",
    "                    age = dam['yrc'][dup]\n",
    "                    dup = [dup[idx] for idx in (np.where(age == np.min(age))[0])]\n",
    "                        \n",
    "                    if len(dup)>1: # If dam ages are the same, take the largest surface area\n",
    "                        SA = dam['SA_m2'][dup]\n",
    "                        dup = [dup[idx] for idx in (np.where(SA==np.max(SA))[0])]\n",
    "                            \n",
    "                        if len(dup) > 1:  # If the surface areas are the same, take the largest dam length arbitrarily\n",
    "                            length = dam['Dam_Len_m'][dup]\n",
    "                            dup = [dup[idx] for idx in (np.where(length==np.max(length))[0])]\n",
    "                                \n",
    "                            if len(dup)>1: # If dam lengths are the same, take any that aren't privately owned.\n",
    "                                owner = dam['OwnerTypes'][dup]\n",
    "                                if (owner == 'Private').all():\n",
    "                                    dup = dup  # Keep all indices if all are 'Private'\n",
    "                                else:\n",
    "                                    non_private_idx = owner.index[owner != 'Private'].tolist()\n",
    "                                    dup = [idx for idx in dup if idx in non_private_idx]\n",
    "                                            \n",
    "                                if len(dup)>1:\n",
    "                                    SID = dam['ShortID'][dup]\n",
    "                                    dup = dup[np.where(SID == np.max(SID))[0][0]]\n",
    "                                \n",
    "                                else:\n",
    "                                    dup = dup[0]\n",
    "                            else:\n",
    "                                dup = dup[0]\n",
    "                        else:\n",
    "                            dup = dup[0]\n",
    "                    else:\n",
    "                        dup = dup[0]\n",
    "                else:\n",
    "                    dup = dup[0]\n",
    "            else:\n",
    "                dup = dup[0]\n",
    "\n",
    "            dupind.append(dup)\n",
    "            \n",
    "\n",
    "\n",
    "# Update dam to only keep the selected non-duplicate flowline snaps\n",
    "dam = dam.iloc[dupind]\n",
    "dam = dam.reset_index(drop=True)\n",
    "flags = [flags[i] for i in dupind]\n",
    "\n",
    "\n",
    "print('Length after removing duplicate snaps to same hydrosequence:',len(dam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "509e411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variables and empty datasets\n",
    "ToDam = np.full(len(dam['ShortID']),np.nan)\n",
    "FromDam = [None]*(len(dam['ShortID']))\n",
    "DA = np.full(len(ToDam),np.nan)\n",
    "flagDA = np.full(len(ToDam),0)\n",
    "flagTerm = np.full(len(ToDam),0)\n",
    "flagHW = np.full(len(ToDam),0)\n",
    "flagCAP = np.full(len(ToDam),0)\n",
    "countryOut = np.full(len(ToDam),0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42843dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dam_hydroseq_array = dam.Hydroseq.to_numpy() # Convert to numpy array due to pandas equality issues\n",
    "\n",
    "for i in range(len(dam.Hydroseq)):\n",
    "\n",
    "    count = 0\n",
    "    where = np.where(NHD.Hydroseq == dam.Hydroseq[i])[0][0] # Find the location of the dam in the NHD table\n",
    "    d_s = NHD.DnHydroseq[where] # Look to see what the downstream flowpath is\n",
    "\n",
    "\n",
    "    locCoast = np.where(NHD.Hydroseq == dam.TerminalPa[i])[0][0]\n",
    "    country_Out = NHD.Country_out[locCoast]\n",
    "    countryOut[i] = country_Out\n",
    "    \n",
    "    if len([locCoast]) == 0:\n",
    "        countryOut[i] = -999 # If there wasn't a match between the Hydroseq and terminal path\n",
    "\n",
    "    while d_s not in dam_hydroseq_array: # While the current flowpath does not have a dam on it\n",
    "        flag = 0 # Reset flag\n",
    "        \n",
    "        if count > len(NHD.Hydroseq): # If you have looped through all of the NHD table; this is one way to ID terminal dams, but could be not actually terminal if NHD flowline is missing\n",
    "            flagTerm = 1\n",
    "            break\n",
    "        elif d_s == 0.0: # No downstream flowpath\n",
    "            flag = 2\n",
    "            break\n",
    "        elif np.isnan(d_s):\n",
    "            flag = 3\n",
    "            break\n",
    "        else:\n",
    "            where = np.where(NHD.Hydroseq == d_s)[0][0] # Update the current flowpath\n",
    "            \n",
    "            if len([where]) == 0: # The d_s hydrosequence is missing from NHD\n",
    "                flag = 4\n",
    "                break\n",
    "                \n",
    "            if NHD.FCODE[where] == 56600: # If you made it to a coastline, add a flag and break.\n",
    "                flag = 10\n",
    "                break\n",
    "                \n",
    "            d_s = NHD.DnHydroseq[where]\n",
    "            count += 1\n",
    "                \n",
    "    \n",
    "    if flag > 0: # If we had a flag already, assign it to the final flags array\n",
    "        if flags[i] is None:\n",
    "            flags[i] = [flag]\n",
    "        else:\n",
    "            flags[i].append(flag)\n",
    "        continue\n",
    "    else: # Otherwise the flag was 0 and nothing abnormal happened, or the flag was 5, which means it originally had a duplicate dam on a hydrosequence\n",
    "        damloc = np.where(dam.Hydroseq == d_s)[0][0] # Find which dam we reached\n",
    "        \n",
    "        ToDam[i] = dam.ShortID[damloc] # Update the ToDam column\n",
    "        \n",
    "        if FromDam[damloc] is None:\n",
    "            FromNow = [dam.ShortID[i]]\n",
    "        else:\n",
    "            FromNow = FromDam[damloc] # Update FromDam\n",
    "            FromNow.append(dam.ShortID[i])\n",
    "        \n",
    "        FromDam[damloc] = FromNow\n",
    "        \n",
    "        if flags[i] is None:\n",
    "            flags[i] = [flag]\n",
    "        else:\n",
    "            flags[i].append(flag)\n",
    "        \n",
    "        # Add flag if DA d/s is smaller than DA of u/s dam\n",
    "        if dam.DivDASqKM[damloc] < dam.DivDASqKM[i]:\n",
    "            flagDA[i] = 6\n",
    "            flagDA[damloc] = 6\n",
    "            if flags[i] is None:\n",
    "                flags[i] = 6\n",
    "            else:\n",
    "                flags[i].append(6)\n",
    "            if flags[damloc] is None:\n",
    "                flags[damloc] = [6]\n",
    "            else:\n",
    "                flags[damloc].append(6)\n",
    "        elif dam.DivDASqKM[damloc] == dam.DivDASqKM[i]: # Drainage areas are equal\n",
    "            flagDA[i] = 7\n",
    "            flagDA[damloc] = 7\n",
    "            if flags[i] is None:\n",
    "                flags[i] = [7]\n",
    "            else:\n",
    "                flags[i].append(7)\n",
    "            if flags[damloc] is None:\n",
    "                flags[damloc] = [7]\n",
    "            else:\n",
    "                flags[damloc].append(7)\n",
    "                \n",
    "        if dam.MaxStor_m3[damloc] == dam.MaxStor_m3[i]: # Flag if capacity the same at two subsequent dams\n",
    "            flagCAP[i] = 8\n",
    "            flagCAP[damloc] = 8\n",
    "            if flags[i] is None:\n",
    "                flags[i] = [8]\n",
    "            else:\n",
    "                flags[i].append(8)\n",
    "            if flags[damloc] is None:\n",
    "                flags[damloc] = [8]\n",
    "            else:\n",
    "                flags[damloc].append(8)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c070753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark headwater dams\n",
    "hw = [i for i, value in enumerate(FromDam) if value is None] # Find indices where FromDam is None\n",
    "flagHW[hw] = 1\n",
    "\n",
    "\n",
    "# Iterate over the identified indices\n",
    "for i in hw:\n",
    "    if flags[i] is None:\n",
    "        flags[i] = [9]\n",
    "    else:\n",
    "        flags[i].append(9)\n",
    "\n",
    "        \n",
    "# Mark terminal dams\n",
    "term = np.where(np.isnan(ToDam))[0]\n",
    "flagTerm[term] = 1\n",
    "\n",
    "for i in term:\n",
    "    if flags[i] is None:\n",
    "        flags[i] = 1\n",
    "    else:\n",
    "        flags[i].append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "729fdae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add fields to the dataframe\n",
    "dam['FromDam'] = FromDam\n",
    "dam['ToDam'] = ToDam\n",
    "dam['flag'] = flags\n",
    "dam['flagDA'] = flagDA\n",
    "dam['flagCAP'] = flagCAP\n",
    "dam['flagTerm'] = flagTerm\n",
    "dam['flagHW'] = flagHW\n",
    "dam['countryOut'] = countryOut\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f984c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define site tags function\n",
    "def assignSiteTags(siteNow, allDams, upstreamDamID, tag_field):\n",
    "\n",
    "    if siteNow['FromDam'] is not None:\n",
    "        FromDam = siteNow.FromDam.iloc[0]\n",
    "    else:\n",
    "        FromDam = None \n",
    "    \n",
    "    if FromDam:\n",
    "        for dam_id in FromDam:\n",
    "            idx = allDams.index[allDams['ShortID'] == dam_id].tolist()\n",
    "            \n",
    "            if idx:\n",
    "                allDams.loc[idx[0], tag_field] = upstreamDamID\n",
    "                site_now_next = allDams.loc[idx]\n",
    "                allDams = assignSiteTags(site_now_next, allDams, upstreamDamID,tag_field)\n",
    "    \n",
    "    return allDams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02fb385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add site tags\n",
    "# Sort the dataframe by ascending Hydrosequence\n",
    "dam = dam.sort_values('Hydroseq', ascending = True)\n",
    "\n",
    "# Pull out sites\n",
    "sites = dam.loc[(dam.IsSite == 1) & (dam.IsRiverMth == 0)]\n",
    "sites = sites.reset_index(drop=True)\n",
    "dam['SiteTag'] = np.full(len(ToDam),0)\n",
    "\n",
    "for i in range(len(sites.IsSite)):\n",
    "    siteNow = sites.iloc[[i]]\n",
    "    tag = siteNow.ShortID.iloc[0]\n",
    "    \n",
    "    dam = assignSiteTags(siteNow,dam,tag,'SiteTag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1fa4e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add GRanD tag\n",
    "# Pull out GRanD\n",
    "GRanD = dam.loc[dam.IsGRanD == 1]\n",
    "GRanD = GRanD.reset_index(drop=True)\n",
    "\n",
    "dam['GRanDTag'] = np.full(len(ToDam),0)\n",
    "\n",
    "for i in range(len(GRanD.IsGRanD)):\n",
    "    GRanDNow = GRanD.iloc[[i]]\n",
    "    tag = GRanDNow.ShortID.iloc[0]\n",
    "    \n",
    "    dam = assignSiteTags(GRanDNow,dam,tag,'GRanDTag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ceb3797e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add river tag\n",
    "# Pull out rivers\n",
    "river = dam.loc[dam.IsRiverMth == 1]\n",
    "river = river.reset_index(drop=True)\n",
    "\n",
    "dam['RiverTag'] = np.full(len(ToDam),0)\n",
    "\n",
    "for i in range(len(river.IsRiverMth)):\n",
    "    riverNow = river.iloc[[i]]\n",
    "    tag = riverNow.ShortID.iloc[0]\n",
    "    \n",
    "    dam = assignSiteTags(riverNow,dam,tag,'RiverTag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1d0df87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add delta tag\n",
    "# Pull out deltas\n",
    "delta = dam.loc[dam.delta > 0]\n",
    "delta = delta.reset_index(drop=True)\n",
    "\n",
    "dam['DeltaTag'] = np.full(len(ToDam),0)\n",
    "\n",
    "for i in range(len(delta.delta)):\n",
    "    deltaNow = delta.iloc[[i]]\n",
    "    tag = deltaNow.delta.iloc[0]\n",
    "    \n",
    "    dam = assignSiteTags(deltaNow,dam,tag,'DeltaTag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11c9f154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete extra columns\n",
    "dam = dam.drop(['FCODE','DnHydroseq','Country_ou','TerminalPa','LENGTHKM','MaxQ_m3s','DA_km2','flagDA',\n",
    "               'flagCAP','Pathlength','WBCOMID','SA_m2','PrimDamTyp','Reservoir'],axis=1)\n",
    "\n",
    "dam = dam.rename(columns={'NrX_Final':'Longitude','NrY_Final':'Latitude','GRAND_ID':'GRanD_ID'})\n",
    "\n",
    "# Save as a csv\n",
    "dam.to_csv(outputFile,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ea0bec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50438292",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:resnet]",
   "language": "python",
   "name": "conda-env-resnet-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
